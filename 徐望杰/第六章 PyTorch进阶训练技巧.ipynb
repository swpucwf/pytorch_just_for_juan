{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第六章 PyTorch进阶训练技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 自定义损失函数\n",
    "随着深度学习的发展, 传统的在`torch.nn`中收录的损失函数慢慢无法完全满足我们的需求. 对于一下非通用的模型, 有许多独特的损失函数需要我们自定义损失函数来提升模型效果.\n",
    "\n",
    "因此学习自定义损失函数是非常重要的一部分\n",
    "\n",
    "### 6.1.1 以函数定义\n",
    "损失函数同样是一个函数, 因此可以以函数的形式直接定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(output, target):\n",
    "    loss = torch.mean((output-target)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 以类定义\n",
    "我们可以把损失函数看作神经网络的一层, 通过```nn.Module```来定义损失函数类.\n",
    "\n",
    "我们如果看每一个损失函数的继承关系我们就可以发现`Loss`函数部分继承自`_loss`, 部分继承自`_WeightedLoss`, 而`_WeightedLoss`继承自`_loss`, ` _loss`继承自 **nn.Module**.\n",
    "\n",
    "以DiceLoss的实现为例进行说明\n",
    "\n",
    "DiceLoss的定义如下:\n",
    "$$\n",
    "DSC = \\frac{2|X∩Y|}{|X|+|Y|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight = None, size_average = True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = F.sigmoid(inputs)\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targetsts.sum() + smooth)\n",
    "        return 1-dice\n",
    "    \n",
    "criterion = DiceLoss()\n",
    "# inputs = 0\n",
    "# targets = 0\n",
    "# loss = criterion(inputs, targets)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意事项\n",
    "在涉及到数学运算的过程时, 最好使用PyTorch的张量计算接口, 这样就可以实现自动求导功能并可以直接调用cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 动态调整学习率\n",
    "在深度学习中, 一个固定的学习速率会影响后期的训练效果. 在多轮训练后, 就会出现准确率震荡或者loss不再下降的情况.\n",
    "\n",
    "因此, 我们可以设定一种合理的学习率衰减策略来改善现象, 以此提高精度. 这种方式被称为scheduler.\n",
    "\n",
    "### 6.2.1 使用官方scheduler\n",
    "官方的scheduler封装在`torch.optim.lr_scheduler`模块中\n",
    "\n",
    "下为官方的实例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_27164/3568393739.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\win-10\\AppData\\Local\\Temp/ipykernel_27164/3568393739.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    scheduler1 = torch.optim.lr_scheduler....\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 选择一种优化器\n",
    "optimizer = torch.optim.Adam(...) \n",
    "# 选择上面提到的一种或多种动态调整学习率的方法\n",
    "scheduler1 = torch.optim.lr_scheduler.... \n",
    "scheduler2 = torch.optim.lr_scheduler....\n",
    "...\n",
    "schedulern = torch.optim.lr_scheduler....\n",
    "# 进行训练\n",
    "for epoch in range(100):\n",
    "    train(...)\n",
    "    validate(...)\n",
    "    optimizer.step()\n",
    "    # 需要在优化器参数更新之后再动态调整学习率\n",
    "\tscheduler1.step() \n",
    "\t...\n",
    "    schedulern.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 自定义scheduler\n",
    "我们同样可以自定义函数进行学习速率的调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个每三十轮下降为原来的1/10\n",
    "def adjust_learning_rate(optimizer,epoch):\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后通过这个函数, 我们可以调用函数实现学习率的变化, 具体方式同调用官方的方法."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 模型微调\n",
    "### 6.3.1 背景知识\n",
    "#### 为什么要模型微调\n",
    "对于很多应用场景, 一个经典的固定模型都会因为数据集和模型的不契合产生过拟合等现象. 但是要重新收集相匹配的数据又非常困难, 因此我们可以通过将源数据集学到的知识迁移到目标数据集上. 这种方法是**迁移学习**的一种应用.\n",
    "\n",
    "因此, 我们就可以通过找一个同类的模型, 将模型通过训练调整一下参数, 然后就可以应用在自己的场景上.\n",
    "\n",
    "PyTorch中有许多预训练好的网络模型(VGG, ResNet系列, mobilenet系列)\n",
    "\n",
    "### 6.3.2 模型微调的流程\n",
    "1. 拥有一个预训练完成的网络模型, 又称源模型\n",
    "2. 创建一个目标模型, 它复制了源模型上除了输出层歪的所有模型设计和参数.\n",
    "3. 为目标模型添加一个输出层, 这个输出层需要适应目标数据集, 然后随机初始化模型参数\n",
    "4. 训练目标模型, 将输出层完全训练完后, 再微调其余层的参数即可\n",
    "\n",
    "![finetune](./figures/finetune.png)\n",
    "\n",
    "### 6.3.3 使用已有模型的注意事项\n",
    "1. PyTorch模型扩展为`.pt`或`.pth`\n",
    "2. 我们可以在[下载网址](https://github.com/pytorch/vision/tree/master/torchvision/models)中查看自己的模型里面`model_urls`, 然后手动下载.\n",
    "3. 默认下载地址是在`Windows`下就是`C:\\Users\\<username>\\.cache\\torch\\hub\\checkpoint`. 我们可以通过使用 [`torch.utils.model_zoo.load_url()`](https://pytorch.org/docs/stable/model_zoo.html#torch.utils.model_zoo.load_url)设置权重的下载地址.\n",
    "\n",
    "### 6.3.4 训练特定层\n",
    "在我们只想训练新加入的初始化层的情况下, 我们就可以通过设置`requires_grad = False`来冻结部分层."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样的话就可以达成只训练指定模型的特定层的目标."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 半精度训练\n",
    "半精度计算可以减少显存占用. 使得显卡可以同时加载更多数据进行计算.\n",
    "\n",
    "#### 6.4.1 半精度训练的设置\n",
    "- import autocast\n",
    "- 模型设置\n",
    "    - 使用装饰器的方法进行forward函数的装饰\n",
    "- 训练过程放在`with autocast()`后面"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
