{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff46115",
   "metadata": {},
   "source": [
    "# PyTorch进阶训练技巧\n",
    "\n",
    "## 1. 自定义损失函数\n",
    "## 2. 动态调整学习率\n",
    "## 3. 模型微调\n",
    "## 4. 半精度学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b228fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caedfe7a",
   "metadata": {},
   "source": [
    "## 1. 自定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61d61605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以函数的方式定义损失函数,通过输出值和目标值进行计算，返回损失值\n",
    "def my_loss(output,target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ee886e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss为： tensor(0.2933, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y_pred = torch.tensor(np.array([[0.2, 0.5], [0.3, 0.1], [0.9, 0.6]]))\n",
    "y = torch.tensor(np.array([[0, 1], [1, 0], [0, 1]]))\n",
    "print('loss为：',my_loss(y_pred,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "261a66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以类方式定义：通过继承nn.Module，将其当作神经网络的一层来看待。\n",
    "# 以DiceLoss损失函数为列子\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss,self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = F.sigmoid(inputs)\n",
    "        # 将输入和输出转化为一维\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        # 实现点成的效果\n",
    "        intersection = (inputs * targets).sum()                   \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        return 1 - dice\n",
    "\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(IoULoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        total = (inputs + targets).sum()\n",
    "        union = total - intersection \n",
    "        \n",
    "        IoU = (intersection + smooth)/(union + smooth)\n",
    "                \n",
    "        return 1 - IoU\n",
    "\n",
    "\n",
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        BCE_EXP = torch.exp(-BCE)\n",
    "        focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n",
    "                       \n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "766517f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_loss为： tensor(0.3858, dtype=torch.float64)\n",
      "I_loss为： tensor(0.5087, dtype=torch.float64)\n",
      "F_loss为： tensor(0.1459, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "D_loss = DiceLoss()\n",
    "I_loss = IoULoss()\n",
    "F_loss = FocalLoss()\n",
    "y_pred = torch.tensor(np.array([[0.2, 0.5], [0.3, 0.1], [0.9, 0.6]]))\n",
    "y = torch.tensor(np.array([[0, 1], [1.0, 0.0], [0.0, 1.0]]))\n",
    "print('D_loss为：',D_loss(y_pred,y))\n",
    "print('I_loss为：',I_loss(y_pred,y))\n",
    "print('F_loss为：',F_loss(y_pred,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe985c3",
   "metadata": {},
   "source": [
    "## 2. 动态调整学习率\n",
    "\n",
    "- 学习率设置小了，会降低收敛速度，增加训练时间；\n",
    "- 学习率设置大了，有可能导致在最优解两侧来回震荡；\n",
    "- PyTorch中的scheduler，提供了解决问题的策略；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a618f69",
   "metadata": {},
   "source": [
    "- PyTorch scheduler策略\n",
    "\n",
    "\n",
    "* [lr_scheduler.LambdaLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR)\n",
    "* [lr_scheduler.MultiplicativeLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR)\n",
    "* [lr_scheduler.StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR)\n",
    "* [lr_scheduler.MultiStepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR)\n",
    "* [lr_scheduler.ExponentialLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR)\n",
    "* [lr_scheduler.CosineAnnealingLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR)\n",
    "* [lr_scheduler.ReduceLROnPlateau](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau)\n",
    "* [lr_scheduler.CyclicLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR)\n",
    "* [lr_scheduler.OneCycleLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR)\n",
    "* [lr_scheduler.CosineAnnealingWarmRestarts](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b794454c",
   "metadata": {},
   "source": [
    "**使用说明**：需要将`scheduler.step()`放在`optimizer.step()`后面"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59567448",
   "metadata": {},
   "source": [
    "**1.使用官方API的动态调整学习率**：\n",
    "```python\n",
    "# 选择一种优化器\n",
    "optimizer = torch.optim.Adam(...) \n",
    "# 选择上面提到的一种或多种动态调整学习率的方法\n",
    "scheduler1 = torch.optim.lr_scheduler.... \n",
    "scheduler2 = torch.optim.lr_scheduler....\n",
    "...\n",
    "schedulern = torch.optim.lr_scheduler....\n",
    "# 进行训练\n",
    "for epoch in range(100):\n",
    "    train(...)\n",
    "    validate(...)\n",
    "    optimizer.step()\n",
    "    # 需要在优化器参数更新之后再动态调整学习率\n",
    "\t   scheduler1.step() \n",
    "\t    ...\n",
    "    schedulern.step()\n",
    "```\n",
    "**2.自定义scheduler**\n",
    "```python\n",
    "# adjust_learning_rate 自定义的学习率\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = args.lr,momentum = 0.9)\n",
    "for epoch in range(10):\n",
    "    train(...)\n",
    "    validate(...)\n",
    "    adjust_learning_rate(optimizer,epoch)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac2d9dc",
   "metadata": {},
   "source": [
    "# 3. 模型微调\n",
    "\n",
    " - 概念： 找一个同类训练好的模型，修改其输出层，冻结输出层之前的权重，训练输出层的参数；\n",
    " - 模型微调的流程：\n",
    " 1. 在源数据集上训练神经网络模型，得到源模型；\n",
    " 2. 创建一个新的神经网络模型，即目标模型，目标模型 = 源模型（结构+权重，结构不包括输出层）+ 新创建的输出层；\n",
    " 3. 新创建的输出层，为目标数据集类别个数的输出层，并随机初始化新输出层的参数；\n",
    " 4. 使用目标数据集训练目标模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f4157d",
   "metadata": {},
   "source": [
    "![jupyter](./模型微调.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21cb3dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原输出层：\n",
      " Linear(in_features=2048, out_features=1000, bias=True)\n",
      "现输出层：\n",
      " Linear(in_features=512, out_features=4, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# 冻结部分网络层\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "\n",
    "import torchvision.models as models\n",
    "# 冻结参数的梯度\n",
    "feature_extract = True\n",
    "model = models.resnet50(pretrained=True)\n",
    "set_parameter_requires_grad(model, feature_extract)\n",
    "print('原输出层：\\n',model.fc)\n",
    "model.fc = nn.Linear(in_features=512, out_features=4, bias=True)\n",
    "print('现输出层：\\n',model.fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119955ca",
   "metadata": {},
   "source": [
    "## 4.半精度训练\n",
    "- “半精度”：PyTorch默认的浮点数存储方式用的是`torch.float32`，其精度高，但是大数情况无需那么高，`torch.float16`的即可。\n",
    "\n",
    "\n",
    "\n",
    "- 半精度训练的优点：减少显存占用，使得显卡可以同时加载更多数据进行计算。\n",
    "\n",
    "\n",
    "\n",
    "- 设置半精度训练:\n",
    "  1. 导入`torch.cuda.amp`的`autocast`包\n",
    "    ```pyhton\n",
    "    from torch.cuda.amp import autocast\n",
    "    ```\n",
    "  2. 在模型定义中的`forwar`d函数上，设置`autocast`装饰器\n",
    "    ```pyhton\n",
    "    @autocast()   \n",
    "    def forward(self, x):\n",
    "    ...\n",
    "    return x\n",
    "    ```\n",
    "  3. 在训练过程中，在数据输入模型之后，添加`with autocast()`\n",
    "  ```pyhton\n",
    "   for x in train_loader:\n",
    "\tx = x.cuda()\n",
    "\twith autocast():\n",
    "        output = model(x)\n",
    "        ...\n",
    "  \n",
    "  ```\n",
    "\n",
    "- 适用范围：适用于数据的size较大的数据集（比如3D图像、视频等）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a665e9e",
   "metadata": {},
   "source": [
    "# 总结\n",
    "&ensp;&ensp;本次任务，主要介绍了Pytorch的进阶技巧：自定义损失函数、动态调整学习率、模型微调和半精度训练技巧。\n",
    "\n",
    "\n",
    " 1. 自定义损失函数：函数方式和类方式，建议一般从输入到输出一直使用张量计算，不然过程会出现冲突错误；\n",
    " 2. 动态调整学习率：相对小学习率提高了速度，相对大学习率避免了最优出横跳，yTorch中的scheduler动态调整学习率，也支持自定义scheduler；\n",
    " 3. 模型微调：目标模型 = 源模型（结构(不包括输出层）和 参数）+ 新输出层，在新数据集上进行训练；\n",
    " 4. 半精度训练主要适用于数据的size较大的数据集（比如3D图像、视频等）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb7e06",
   "metadata": {},
   "source": [
    "**注意**：在训练过程中，model仍会回传梯度，但是参数更新只会发生在`fc`层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8889802c",
   "metadata": {},
   "source": [
    "**参考**：\n",
    "\n",
    "https://blog.csdn.net/weixin_44696221/article/details/104484355 \n",
    "\n",
    "https://relph1119.github.io/my-team-learning/#/pytorch_learning35/task05\n",
    "\n",
    "https://github.com/datawhalechina/thorough-pytorch/blob\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
